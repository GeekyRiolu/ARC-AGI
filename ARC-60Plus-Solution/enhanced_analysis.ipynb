{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¨ Advanced ARC Dataset Analysis for 60%+ Score\n",
    "\n",
    "Comprehensive analysis to identify patterns and optimize solution components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "from scipy import ndimage\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üî¨ Advanced ARC Dataset Analysis Initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load comprehensive ARC data\n",
    "def load_all_arc_data():\n",
    "    \"\"\"Load all ARC datasets for comprehensive analysis\"\"\"\n",
    "    data_path = '/kaggle/input/arc-prize-2025'  # Adjust path as needed\n",
    "    \n",
    "    datasets = {}\n",
    "    \n",
    "    # Load all available datasets\n",
    "    files_to_load = [\n",
    "        ('train_challenges', 'arc-agi_training_challenges.json'),\n",
    "        ('train_solutions', 'arc-agi_training_solutions.json'),\n",
    "        ('eval_challenges', 'arc-agi_evaluation_challenges.json'),\n",
    "        ('eval_solutions', 'arc-agi_evaluation_solutions.json'),\n",
    "        ('test_challenges', 'arc-agi_test_challenges.json')\n",
    "    ]\n",
    "    \n",
    "    for key, filename in files_to_load:\n",
    "        try:\n",
    "            with open(f'{data_path}/{filename}', 'r') as f:\n",
    "                datasets[key] = json.load(f)\n",
    "            print(f\"‚úÖ Loaded {key}: {len(datasets[key])} items\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"‚ö†Ô∏è  {filename} not found, skipping\")\n",
    "            datasets[key] = {}\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# Load data\n",
    "arc_datasets = load_all_arc_data()\n",
    "print(f\"\\nüìä Total datasets loaded: {len([k for k, v in arc_datasets.items() if v])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Advanced transformation pattern analyzer\n",
    "class TransformationAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.patterns = defaultdict(int)\n",
    "        self.pattern_examples = defaultdict(list)\n",
    "        self.complexity_scores = {}\n",
    "        \n",
    "    def analyze_transformation(self, input_grid, output_grid, task_id, example_id):\n",
    "        \"\"\"Analyze a single input->output transformation\"\"\"\n",
    "        inp = np.array(input_grid)\n",
    "        out = np.array(output_grid)\n",
    "        \n",
    "        patterns_found = []\n",
    "        \n",
    "        # 1. Identity transformation\n",
    "        if np.array_equal(inp, out):\n",
    "            patterns_found.append('identity')\n",
    "        \n",
    "        # 2. Size changes\n",
    "        if inp.shape != out.shape:\n",
    "            if out.shape[0] > inp.shape[0] or out.shape[1] > inp.shape[1]:\n",
    "                patterns_found.append('size_increase')\n",
    "            else:\n",
    "                patterns_found.append('size_decrease')\n",
    "        \n",
    "        # 3. Geometric transformations\n",
    "        if inp.shape == out.shape:\n",
    "            # Rotation checks\n",
    "            for k in [1, 2, 3]:\n",
    "                if np.array_equal(np.rot90(inp, k), out):\n",
    "                    patterns_found.append(f'rotation_{k*90}')\n",
    "                    break\n",
    "            \n",
    "            # Reflection checks\n",
    "            if np.array_equal(np.fliplr(inp), out):\n",
    "                patterns_found.append('reflection_horizontal')\n",
    "            elif np.array_equal(np.flipud(inp), out):\n",
    "                patterns_found.append('reflection_vertical')\n",
    "        \n",
    "        # 4. Color transformations\n",
    "        inp_colors = set(np.unique(inp))\n",
    "        out_colors = set(np.unique(out))\n",
    "        \n",
    "        if inp_colors != out_colors:\n",
    "            if len(out_colors) > len(inp_colors):\n",
    "                patterns_found.append('color_addition')\n",
    "            elif len(out_colors) < len(inp_colors):\n",
    "                patterns_found.append('color_reduction')\n",
    "            else:\n",
    "                patterns_found.append('color_substitution')\n",
    "        \n",
    "        # 5. Fill operations\n",
    "        if np.sum(inp == 0) > np.sum(out == 0):\n",
    "            patterns_found.append('fill_background')\n",
    "        \n",
    "        # 6. Pattern completion\n",
    "        if self._is_pattern_completion(inp, out):\n",
    "            patterns_found.append('pattern_completion')\n",
    "        \n",
    "        # 7. Object operations\n",
    "        inp_objects = self._count_objects(inp)\n",
    "        out_objects = self._count_objects(out)\n",
    "        \n",
    "        if out_objects > inp_objects:\n",
    "            patterns_found.append('object_duplication')\n",
    "        elif out_objects < inp_objects:\n",
    "            patterns_found.append('object_removal')\n",
    "        \n",
    "        # 8. Symmetry operations\n",
    "        if self._creates_symmetry(inp, out):\n",
    "            patterns_found.append('symmetry_creation')\n",
    "        \n",
    "        # 9. Counting operations\n",
    "        if self._involves_counting(inp, out):\n",
    "            patterns_found.append('counting_operation')\n",
    "        \n",
    "        # 10. Line/connection operations\n",
    "        if self._involves_connections(inp, out):\n",
    "            patterns_found.append('connection_operation')\n",
    "        \n",
    "        # Record patterns\n",
    "        for pattern in patterns_found:\n",
    "            self.patterns[pattern] += 1\n",
    "            if len(self.pattern_examples[pattern]) < 5:  # Store up to 5 examples\n",
    "                self.pattern_examples[pattern].append({\n",
    "                    'task_id': task_id,\n",
    "                    'example_id': example_id,\n",
    "                    'input': input_grid,\n",
    "                    'output': output_grid\n",
    "                })\n",
    "        \n",
    "        return patterns_found\n",
    "    \n",
    "    def _is_pattern_completion(self, inp, out):\n",
    "        \"\"\"Check if transformation completes a pattern\"\"\"\n",
    "        # Simple heuristic: more non-zero cells in output\n",
    "        return np.sum(out != 0) > np.sum(inp != 0) * 1.2\n",
    "    \n",
    "    def _count_objects(self, grid):\n",
    "        \"\"\"Count connected components (objects) in grid\"\"\"\n",
    "        try:\n",
    "            labeled, num_features = ndimage.label(grid != 0)\n",
    "            return num_features\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    def _creates_symmetry(self, inp, out):\n",
    "        \"\"\"Check if output is more symmetric than input\"\"\"\n",
    "        # Check horizontal symmetry\n",
    "        out_h_sym = np.array_equal(out, np.fliplr(out))\n",
    "        inp_h_sym = np.array_equal(inp, np.fliplr(inp))\n",
    "        \n",
    "        # Check vertical symmetry\n",
    "        out_v_sym = np.array_equal(out, np.flipud(out))\n",
    "        inp_v_sym = np.array_equal(inp, np.flipud(inp))\n",
    "        \n",
    "        return (out_h_sym and not inp_h_sym) or (out_v_sym and not inp_v_sym)\n",
    "    \n",
    "    def _involves_counting(self, inp, out):\n",
    "        \"\"\"Check if transformation involves counting\"\"\"\n",
    "        # Heuristic: small output with values that could be counts\n",
    "        if out.size <= 9 and np.max(out) <= 9:\n",
    "            unique_inp = len(np.unique(inp[inp != 0]))\n",
    "            return unique_inp > 0 and np.any(out == unique_inp)\n",
    "        return False\n",
    "    \n",
    "    def _involves_connections(self, inp, out):\n",
    "        \"\"\"Check if transformation involves connecting elements\"\"\"\n",
    "        # Heuristic: more connected components in output\n",
    "        inp_edges = np.sum(np.diff(inp, axis=0) != 0) + np.sum(np.diff(inp, axis=1) != 0)\n",
    "        out_edges = np.sum(np.diff(out, axis=0) != 0) + np.sum(np.diff(out, axis=1) != 0)\n",
    "        return out_edges > inp_edges * 1.5\n",
    "\n",
    "print(\"üîç Transformation analyzer initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Run comprehensive analysis\n",
    "def run_comprehensive_analysis():\n",
    "    \"\"\"Run complete dataset analysis\"\"\"\n",
    "    analyzer = TransformationAnalyzer()\n",
    "    \n",
    "    # Analyze training data\n",
    "    train_challenges = arc_datasets.get('train_challenges', {})\n",
    "    train_solutions = arc_datasets.get('train_solutions', {})\n",
    "    \n",
    "    print(f\"üî¨ Analyzing {len(train_challenges)} training tasks...\")\n",
    "    \n",
    "    task_stats = {\n",
    "        'grid_sizes': [],\n",
    "        'color_usage': Counter(),\n",
    "        'complexity_scores': [],\n",
    "        'num_examples': [],\n",
    "        'transformation_types': []\n",
    "    }\n",
    "    \n",
    "    for task_id, task in train_challenges.items():\n",
    "        if task_id not in train_solutions:\n",
    "            continue\n",
    "            \n",
    "        task_stats['num_examples'].append(len(task['train']))\n",
    "        \n",
    "        # Analyze each training example\n",
    "        for i, example in enumerate(task['train']):\n",
    "            inp = np.array(example['input'])\n",
    "            out = np.array(example['output'])\n",
    "            \n",
    "            # Grid size analysis\n",
    "            task_stats['grid_sizes'].extend([inp.shape, out.shape])\n",
    "            \n",
    "            # Color usage analysis\n",
    "            for color in np.unique(inp):\n",
    "                task_stats['color_usage'][color] += np.sum(inp == color)\n",
    "            for color in np.unique(out):\n",
    "                task_stats['color_usage'][color] += np.sum(out == color)\n",
    "            \n",
    "            # Transformation analysis\n",
    "            patterns = analyzer.analyze_transformation(inp.tolist(), out.tolist(), task_id, i)\n",
    "            task_stats['transformation_types'].extend(patterns)\n",
    "            \n",
    "            # Complexity score (simple heuristic)\n",
    "            complexity = len(np.unique(inp)) + len(np.unique(out)) + abs(inp.size - out.size)\n",
    "            task_stats['complexity_scores'].append(complexity)\n",
    "    \n",
    "    return analyzer, task_stats\n",
    "\n",
    "# Run analysis\n",
    "print(\"üöÄ Starting comprehensive analysis...\")\n",
    "transformation_analyzer, dataset_stats = run_comprehensive_analysis()\n",
    "print(\"‚úÖ Analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize analysis results\n",
    "def create_analysis_visualizations(analyzer, stats):\n",
    "    \"\"\"Create comprehensive visualizations of dataset analysis\"\"\"\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(20, 15))\n",
    "    fig.suptitle('üî¨ ARC Dataset Comprehensive Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Top transformation patterns\n",
    "    top_patterns = dict(Counter(analyzer.patterns).most_common(15))\n",
    "    axes[0,0].barh(list(top_patterns.keys()), list(top_patterns.values()))\n",
    "    axes[0,0].set_title('Top 15 Transformation Patterns')\n",
    "    axes[0,0].set_xlabel('Frequency')\n",
    "    \n",
    "    # 2. Grid size distribution\n",
    "    size_strings = [f\"{s[0]}x{s[1]}\" for s in stats['grid_sizes']]\n",
    "    size_counts = Counter(size_strings)\n",
    "    top_sizes = dict(size_counts.most_common(10))\n",
    "    axes[0,1].bar(range(len(top_sizes)), list(top_sizes.values()))\n",
    "    axes[0,1].set_xticks(range(len(top_sizes)))\n",
    "    axes[0,1].set_xticklabels(list(top_sizes.keys()), rotation=45)\n",
    "    axes[0,1].set_title('Top 10 Grid Sizes')\n",
    "    axes[0,1].set_ylabel('Frequency')\n",
    "    \n",
    "    # 3. Color usage distribution\n",
    "    colors = list(range(10))\n",
    "    color_counts = [stats['color_usage'].get(c, 0) for c in colors]\n",
    "    bars = axes[0,2].bar(colors, color_counts)\n",
    "    axes[0,2].set_title('Color Usage Distribution')\n",
    "    axes[0,2].set_xlabel('Color')\n",
    "    axes[0,2].set_ylabel('Usage Count')\n",
    "    \n",
    "    # Color the bars with actual colors\n",
    "    color_map = ['black', 'blue', 'red', 'green', 'yellow', 'gray', 'magenta', 'orange', 'lightblue', 'brown']\n",
    "    for bar, color in zip(bars, color_map):\n",
    "        bar.set_color(color)\n",
    "    \n",
    "    # 4. Complexity distribution\n",
    "    axes[1,0].hist(stats['complexity_scores'], bins=20, alpha=0.7, edgecolor='black')\n",
    "    axes[1,0].set_title('Task Complexity Distribution')\n",
    "    axes[1,0].set_xlabel('Complexity Score')\n",
    "    axes[1,0].set_ylabel('Frequency')\n",
    "    \n",
    "    # 5. Number of training examples per task\n",
    "    axes[1,1].hist(stats['num_examples'], bins=range(1, max(stats['num_examples'])+2), alpha=0.7, edgecolor='black')\n",
    "    axes[1,1].set_title('Training Examples per Task')\n",
    "    axes[1,1].set_xlabel('Number of Examples')\n",
    "    axes[1,1].set_ylabel('Number of Tasks')\n",
    "    \n",
    "    # 6. Pattern co-occurrence heatmap\n",
    "    pattern_cooccurrence = defaultdict(lambda: defaultdict(int))\n",
    "    # This would require more complex analysis - simplified for now\n",
    "    axes[1,2].text(0.5, 0.5, 'Pattern Co-occurrence\\n(Advanced Analysis)', \n",
    "                   ha='center', va='center', transform=axes[1,2].transAxes, fontsize=12)\n",
    "    axes[1,2].set_title('Pattern Relationships')\n",
    "    \n",
    "    # 7. Grid size vs complexity scatter\n",
    "    grid_areas = [s[0] * s[1] for s in stats['grid_sizes'][:len(stats['complexity_scores'])]]\n",
    "    axes[2,0].scatter(grid_areas, stats['complexity_scores'], alpha=0.6)\n",
    "    axes[2,0].set_title('Grid Size vs Complexity')\n",
    "    axes[2,0].set_xlabel('Grid Area (cells)')\n",
    "    axes[2,0].set_ylabel('Complexity Score')\n",
    "    \n",
    "    # 8. Pattern frequency pie chart\n",
    "    top_5_patterns = dict(Counter(analyzer.patterns).most_common(5))\n",
    "    other_count = sum(analyzer.patterns.values()) - sum(top_5_patterns.values())\n",
    "    if other_count > 0:\n",
    "        top_5_patterns['Others'] = other_count\n",
    "    \n",
    "    axes[2,1].pie(top_5_patterns.values(), labels=top_5_patterns.keys(), autopct='%1.1f%%')\n",
    "    axes[2,1].set_title('Top Pattern Distribution')\n",
    "    \n",
    "    # 9. Summary statistics\n",
    "    summary_text = f\"\"\"\n",
    "    üìä DATASET SUMMARY\n",
    "    \n",
    "    Total Patterns: {len(analyzer.patterns)}\n",
    "    Most Common: {Counter(analyzer.patterns).most_common(1)[0][0]}\n",
    "    Avg Complexity: {np.mean(stats['complexity_scores']):.1f}\n",
    "    Most Used Color: {stats['color_usage'].most_common(1)[0][0]}\n",
    "    Common Grid Size: {Counter(size_strings).most_common(1)[0][0]}\n",
    "    Avg Examples/Task: {np.mean(stats['num_examples']):.1f}\n",
    "    \"\"\"\n",
    "    \n",
    "    axes[2,2].text(0.1, 0.9, summary_text, transform=axes[2,2].transAxes, \n",
    "                   fontsize=10, verticalalignment='top', fontfamily='monospace')\n",
    "    axes[2,2].set_title('Summary Statistics')\n",
    "    axes[2,2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return top_patterns, top_sizes\n",
    "\n",
    "# Create visualizations\n",
    "print(\"üìä Creating analysis visualizations...\")\n",
    "top_patterns, top_sizes = create_analysis_visualizations(transformation_analyzer, dataset_stats)\n",
    "print(\"‚úÖ Visualizations complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate actionable insights for solution improvement\n",
    "def generate_improvement_insights(analyzer, stats, top_patterns):\n",
    "    \"\"\"Generate specific insights for improving the solution\"\"\"\n",
    "    insights = {\n",
    "        'missing_rules': [],\n",
    "        'optimization_opportunities': [],\n",
    "        'neural_network_targets': [],\n",
    "        'ensemble_improvements': []\n",
    "    }\n",
    "    \n",
    "    # Identify missing rules based on pattern frequency\n",
    "    current_rules = {\n",
    "        'identity', 'fill_background', 'reflection_horizontal', 'reflection_vertical',\n",
    "        'rotation_90', 'rotation_180', 'rotation_270', 'color_substitution'\n",
    "    }\n",
    "    \n",
    "    for pattern, count in top_patterns.items():\n",
    "        if pattern not in current_rules and count > 10:  # Significant frequency\n",
    "            insights['missing_rules'].append({\n",
    "                'pattern': pattern,\n",
    "                'frequency': count,\n",
    "                'priority': 'HIGH' if count > 50 else 'MEDIUM',\n",
    "                'examples': analyzer.pattern_examples.get(pattern, [])[:3]\n",
    "            })\n",
    "    \n",
    "    # Optimization opportunities\n",
    "    if stats['color_usage'][0] > sum(stats['color_usage'].values()) * 0.5:\n",
    "        insights['optimization_opportunities'].append({\n",
    "            'type': 'background_optimization',\n",
    "            'description': 'Background (color 0) dominates - optimize background handling',\n",
    "            'impact': 'HIGH'\n",
    "        })\n",
    "    \n",
    "    # Neural network targets\n",
    "    complex_patterns = [p for p, c in top_patterns.items() \n",
    "                       if 'completion' in p or 'connection' in p or 'counting' in p]\n",
    "    \n",
    "    if complex_patterns:\n",
    "        insights['neural_network_targets'].extend([\n",
    "            {\n",
    "                'pattern': pattern,\n",
    "                'reason': 'Complex pattern requiring learned representations',\n",
    "                'architecture': 'CNN + Attention' if 'completion' in pattern else 'Transformer'\n",
    "            } for pattern in complex_patterns[:5]\n",
    "        ])\n",
    "    \n",
    "    # Ensemble improvements\n",
    "    total_patterns = sum(top_patterns.values())\n",
    "    coverage_estimate = sum(count for pattern, count in top_patterns.items() \n",
    "                           if pattern in current_rules) / total_patterns\n",
    "    \n",
    "    insights['ensemble_improvements'].append({\n",
    "        'current_coverage': f\"{coverage_estimate:.1%}\",\n",
    "        'improvement_potential': f\"{(1-coverage_estimate):.1%}\",\n",
    "        'recommendation': 'Focus on top missing patterns for maximum impact'\n",
    "    })\n",
    "    \n",
    "    return insights\n",
    "\n",
    "# Generate insights\n",
    "print(\"üí° Generating improvement insights...\")\n",
    "improvement_insights = generate_improvement_insights(transformation_analyzer, dataset_stats, top_patterns)\n",
    "\n",
    "# Display insights\n",
    "print(\"\\nüéØ KEY INSIGHTS FOR 60%+ SCORE:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\nüîß MISSING RULES (High Priority):\")\n",
    "for rule in improvement_insights['missing_rules'][:5]:\n",
    "    print(f\"  ‚Ä¢ {rule['pattern']}: {rule['frequency']} occurrences ({rule['priority']} priority)\")\n",
    "\n",
    "print(\"\\nüöÄ OPTIMIZATION OPPORTUNITIES:\")\n",
    "for opt in improvement_insights['optimization_opportunities']:\n",
    "    print(f\"  ‚Ä¢ {opt['type']}: {opt['description']} (Impact: {opt['impact']})\")\n",
    "\n",
    "print(\"\\nüß† NEURAL NETWORK TARGETS:\")\n",
    "for target in improvement_insights['neural_network_targets'][:3]:\n",
    "    print(f\"  ‚Ä¢ {target['pattern']}: {target['reason']} -> {target['architecture']}\")\n",
    "\n",
    "print(\"\\nüìà ENSEMBLE IMPROVEMENTS:\")\n",
    "for imp in improvement_insights['ensemble_improvements']:\n",
    "    print(f\"  ‚Ä¢ Current Coverage: {imp['current_coverage']}\")\n",
    "    print(f\"  ‚Ä¢ Improvement Potential: {imp['improvement_potential']}\")\n",
    "    print(f\"  ‚Ä¢ Recommendation: {imp['recommendation']}\")\n",
    "\n",
    "print(\"\\n‚úÖ Analysis complete! Ready for solution enhancement.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
